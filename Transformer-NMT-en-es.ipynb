{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38ae85f-99b5-44d3-a819-1f4d1437b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, MutableMapping, NamedTuple, Tuple\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "import dataset\n",
    "import model\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafb1b48-29ea-44cb-a00e-866819ab91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_TRAINING = True\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "SEQ_LENGTH = 512\n",
    "GRAD_CLIP_VALUE = 1\n",
    "LOG_EVERY = 50\n",
    "MAX_STEPS = 2000\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22eb6d7-c66b-4bec-86bd-ded34ae95190",
   "metadata": {},
   "source": [
    "# dataset & tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783b40a-4e61-4209-80ce-8d10f5d444f6",
   "metadata": {},
   "source": [
    "## Peek at our data (English-Spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be38fe7a-7836-4491-9073-e6e0189421b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration avacaondata--europarl_en_es_v2-4d7b4c2662fbaead\n",
      "WARNING:datasets.builder:Found cached dataset parquet (/homes/pding/.cache/huggingface/datasets/avacaondata___parquet/avacaondata--europarl_en_es_v2-4d7b4c2662fbaead/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'source_en', 'target_es', '__index_level_0__'],\n",
       "    num_rows: 275203\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"avacaondata/europarl_en_es_v2\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9020eaa4-4afe-431d-8541-bfe7aee98df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 19512,\n",
       " 'source_en': \"Let the Commission go about its business and produce an amended version of the exceptions to heavy metals in two years' time.\\nFinally, I should definitely like to mention the discussion on brominated flame retardants.\\nIt has become a discussion between believers, such as myself, convinced of the harmful effects on the environment and health, and non-believers.\\nWhat I find important is that, for many, this discussion has led to a greater appreciation of the harmfulness of these products.\\nA ban in 2006 is impossible.\\nThe amendment tabled by my group asks for producers to demonstrate by 2003 that these products are harmless, and I hope that this can be achieved.\\nMr President, more than thirty years ago an organisation called 'Friends of the Earth' was born in my country.\\n\",\n",
       " 'target_es': 'Dejemos que la Comisión efectúe su trabajo y presente dentro de dos años una versión adecuada de las excepciones en cuanto a metales pesados.\\nPor último, quiero entrar decididamente en la discusión sobre materiales ignífugos bromados que contienen plástico.\\nSe ha convertido en una discusión entre los que, como yo, están convencidos de los efectos perjudiciales para el medio ambiente y la salud, y los que no lo están.\\nConsidero importante que esta discusión haya dado lugar a una mayor concienciación sobre los daños que causan estos productos.\\nNo es posible su prohibición en 2006.\\nEn la enmienda presentada por mi Grupo, se pide que los fabricantes demuestren hacia 2003 que estos productos no son perjudiciales, y espero que eso sí sea factible.\\nSeñor Presidente, hace más de treinta años nació en mi país una organización llamada \"Friends of the Earth\" (Amigos de la Tierra).\\n',\n",
       " '__index_level_0__': 19512}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28e355-d78f-4db9-a20a-0cbe887b9ebf",
   "metadata": {},
   "source": [
    "## Train our tokenizers for English and Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1cd4b-53aa-4418-906e-165f92f63f56",
   "metadata": {},
   "source": [
    "We have prepared the following python code for you:\n",
    "\n",
    "- train_tokenizer_en.py\n",
    "- train_tokenizer_es.py\n",
    "\n",
    "Should you choose to use other language, please change the codes to fit the data format and output directory.\\\n",
    "Running them in separate terminals would be a good idea, as each takes some time to finish.\\\n",
    "For me, it took about 8m19s and 12m42s for English and Spanish tokenizers to train on a server at Argonne.\n",
    "\n",
    "NOTE: non-space-separated languages, such as Chinese, Japanese, Korean, Thai, need further adaptation to the tokenizer pipeline.\n",
    "\n",
    "Useful readings:\n",
    "- https://huggingface.co/docs/tokenizers/quicktour\n",
    "- https://huggingface.co/course/chapter6/8?fw=pt#building-a-tokenizer-block-by-block\n",
    "- https://huggingface.co/docs/tokenizers/index\n",
    "- https://www.reddit.com/r/MachineLearning/comments/rprmq3/d_sentencepiece_wordpiece_bpe_which_tokenizer_is/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bd348-deb3-4889-b1fa-e51dd2cc512b",
   "metadata": {},
   "source": [
    "## Tokenize the text data\n",
    "suppose we have our English and Spanish tokenizers trained and stored at `./vanilla-NMT/en` and `./vanilla-NMT/es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e77de5a-6d21-452e-86f9-be92da3c53f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4477545b-dcbf-4055-ab7d-1d58b9b328c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_english = Tokenizer.from_file(\"vanilla-NMT/en/tokenizer.json\")\n",
    "\n",
    "src_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer_english,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"right\",\n",
    "    truncation_side='right',\n",
    ")\n",
    "\n",
    "tokenizer_spanish = Tokenizer.from_file(\"vanilla-NMT/es/tokenizer.json\")\n",
    "tgt_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer_spanish,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"right\",\n",
    "    truncation_side='right',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d1b391-624f-4580-ac4f-a9f29c4dba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=25007, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e61c222-c53b-45ac-9750-18638f92e0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=25007, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905a2398-3df5-4b9d-be63-d6082d4d8144",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration avacaondata--europarl_en_es_v2-4d7b4c2662fbaead\n"
     ]
    }
   ],
   "source": [
    "# we use streaming version of dataset\n",
    "dataset = load_dataset(\"avacaondata/europarl_en_es_v2\", split='train', streaming=True)\n",
    "\n",
    "# encode function to map on each dataset entry\n",
    "def encode(examples):\n",
    "    def decorate(text, tokenizer):\n",
    "        decorated = f\"{tokenizer.bos_token} {text} {tokenizer.eos_token}\"\n",
    "        decorated = decorated.replace('\\n', tokenizer.sep_token)\n",
    "        return decorated\n",
    "    \n",
    "    src_inputs = src_tokenizer(\n",
    "        decorate(examples['source_en'], src_tokenizer), \n",
    "        truncation=True, max_length=SEQ_LENGTH, padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "    )['input_ids']\n",
    "    tgt_inputs = tgt_tokenizer(\n",
    "        decorate(examples['target_es'], tgt_tokenizer),\n",
    "        truncation=True, max_length=SEQ_LENGTH, padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "    )['input_ids']\n",
    "    return {\n",
    "        'src_inputs': src_inputs,\n",
    "        'tgt_inputs': tgt_inputs,\n",
    "    }\n",
    "\n",
    "# now dataset is a iter object\n",
    "dataset = iter(dataset.map(encode, remove_columns=[\"id\", \"source_en\", \"target_es\", \"__index_level_0__\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e89050cf-8c75-45dd-b559-bdea8ef15866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a761a5-480a-4bfd-964c-505880bf3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the Commission go about its business and produce an amended version of the exceptions to heavy metals in two years' time.  Finally, I should definitely like to mention the discussion on brom inated flame retardants. It has become a discussion between believers, such as myself, convinced of the harmful effects on the environment and health, and non-believers.  What I find important is that, for many, this discussion has led to a greater appreciation of the harmful ness of these products. A ban in 2006 is impossible. The amendment tabled by my group asks for producers to demonstrate by 2003 that these products are harmless, and I hope that this can be achieved. Mr President, more than thirty years ago an organisation called 'F rie nd s of the Earth'was born in my country. \n"
     ]
    }
   ],
   "source": [
    "print(src_tokenizer.decode(next(dataset)['src_inputs'], skip_special_tokens=True).replace('▁', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b9a6eb-a69a-4051-be94-f67ce8da586d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokenizer.decode([3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c101f-3770-4a72-b93f-dcb0490cdbe0",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "`train.py` script is provided. Please run the code in terminal and find your ckpt at `./ckpt` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6fd87-3324-4ee0-aabd-69bab790701b",
   "metadata": {},
   "source": [
    "# Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234a6ead-f22c-46e3-a426-9008c4d429c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import TrainingState, main\n",
    "\n",
    "# some training and model parameters:\n",
    "CONFIG = model.TransformerConfig(\n",
    "    input_vocab_size=src_tokenizer.vocab_size,\n",
    "    output_vocab_size=tgt_tokenizer.vocab_size,\n",
    "    model_size=256,\n",
    "    num_heads=8,\n",
    "    num_layers=6,\n",
    "    hidden_size=512,\n",
    "    dropout_rate=0.1,\n",
    "    src_pad_token=src_tokenizer.pad_token_id,\n",
    "    tgt_pad_token=tgt_tokenizer.pad_token_id,\n",
    ")\n",
    "IS_TRAINING = False\n",
    "\n",
    "# Create the model.\n",
    "def forward(\n",
    "    src_inputs: jnp.ndarray,\n",
    "    tgt_inputs: jnp.ndarray,\n",
    "    is_training: bool,\n",
    ") -> jnp.ndarray:\n",
    "\n",
    "    lm = model.Transformer(\n",
    "        config=CONFIG,\n",
    "        is_training=is_training\n",
    "    )\n",
    "    return lm(src_inputs, tgt_inputs, is_training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e2e0bd7-d01f-469e-bd86-c3883623ff34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "ckpt_file = 'ckpt/state_02-Oct-2022 (01:54:46).pickle'\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open(ckpt_file, 'rb') as handle:\n",
    "    state = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6108391e-b6d7-425d-b27b-7229c22cb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_fn = hk.transform(forward)\n",
    "\n",
    "def predict(src_inputs, tgt_max_len):\n",
    "\n",
    "    src_inputs = jnp.asarray(src_inputs, dtype=jnp.int32)[None,:]\n",
    "    tgt_inputs = jnp.asarray([tgt_tokenizer.bos_token_id], dtype=jnp.int32)[None,:]\n",
    "    \n",
    "    @hk.transform\n",
    "    def one_step(src_inputs, tgt_inputs, is_training=False):\n",
    "        predictions = forward(src_inputs=src_inputs, tgt_inputs=tgt_inputs, is_training=False)\n",
    "        predictions = predictions[:, -1, :]\n",
    "        predicted_id = jnp.argmax(predictions, axis=-1)\n",
    "        return predicted_id\n",
    "        \n",
    "    for i in range(tgt_max_len):\n",
    "        predicted_id = one_step.apply(state.params, state.rng, src_inputs, tgt_inputs, is_training=False)\n",
    "        if predicted_id == tgt_tokenizer.eos_token_id:\n",
    "            return output\n",
    "        tgt_inputs = np.concatenate([tgt_inputs, [predicted_id]], axis=-1)\n",
    "        print(i, tgt_inputs)\n",
    "    return jnp.squeeze(tgt_inputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add98a45-aa0f-4dcc-939d-a8035f4b0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    # Load data (deserialize)\n",
    "    ckpt_file = 'ckpt/state_02-Oct-2022 (01:54:46).pickle'\n",
    "    with open(ckpt_file, 'rb') as handle:\n",
    "        state = pickle.load(handle)\n",
    "    tgt_max_len = 64\n",
    "    src_sentence = f\"{src_tokenizer.bos_token} {sentence} {src_tokenizer.eos_token}\"\n",
    "    src_inputs = src_tokenizer(\n",
    "            src_sentence, \n",
    "            truncation=True, max_length=SEQ_LENGTH, padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_attention_mask=False,\n",
    "        )['input_ids']\n",
    "    \n",
    "    output = predict(src_inputs, tgt_max_len)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad1529f2-8d33-467e-bafc-b93fe7f02303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[5 8]]\n",
      "1 [[   5    8 1529]]\n",
      "2 [[   5    8 1529    9]]\n",
      "3 [[   5    8 1529    9   13]]\n",
      "4 [[   5    8 1529    9   13   17]]\n",
      "5 [[   5    8 1529    9   13   17    9]]\n",
      "6 [[   5    8 1529    9   13   17    9    8]]\n",
      "7 [[   5    8 1529    9   13   17    9    8   14]]\n",
      "8 [[   5    8 1529    9   13   17    9    8   14    9]]\n",
      "9 [[   5    8 1529    9   13   17    9    8   14    9   13]]\n",
      "10 [[   5    8 1529    9   13   17    9    8   14    9   13   15]]\n",
      "11 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109]]\n",
      "12 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9]]\n",
      "13 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13]]\n",
      "14 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15]]\n",
      "15 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9]]\n",
      "16 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7]]\n",
      "17 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17]]\n",
      "18 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9]]\n",
      "19 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13]]\n",
      "20 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17]]\n",
      "21 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9]]\n",
      "22 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13]]\n",
      "23 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17]]\n",
      "24 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9]]\n",
      "25 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13]]\n",
      "26 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17]]\n",
      "27 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7]]\n",
      "28 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17]]\n",
      "29 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9]]\n",
      "30 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13]]\n",
      "31 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17]]\n",
      "32 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9]]\n",
      "33 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8]]\n",
      "34 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14]]\n",
      "35 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9]]\n",
      "36 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13]]\n",
      "37 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17]]\n",
      "38 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9]]\n",
      "39 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13]]\n",
      "40 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10]]\n",
      "41 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9]]\n",
      "42 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13]]\n",
      "43 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17]]\n",
      "44 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9]]\n",
      "45 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13]]\n",
      "46 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17]]\n",
      "47 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9]]\n",
      "48 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13]]\n",
      "49 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17]]\n",
      "50 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9]]\n",
      "51 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13]]\n",
      "52 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15]]\n",
      "53 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7]]\n",
      "54 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17]]\n",
      "55 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9]]\n",
      "56 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13]]\n",
      "57 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17]]\n",
      "58 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17    7]]\n",
      "59 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17    7   17]]\n",
      "60 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17    7   17    9]]\n",
      "61 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17    7   17    9    8]]\n",
      "62 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17    7   17    9    8   14]]\n",
      "63 [[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "    13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "     7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "     9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "     9   13   17    7   17    9    8   14    9]]\n",
      "[   5    8 1529    9   13   17    9    8   14    9   13   15  109    9\n",
      "   13   15    9    7   17    9   13   17    9   13   17    9   13   17\n",
      "    7   17    9   13   17    9    8   14    9   13   17    9   13   10\n",
      "    9   13   17    9   13   17    9   13   17    9   13   15    7   17\n",
      "    9   13   17    7   17    9    8   14    9]\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is a sentence in English.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca88114-a37b-45ff-9c04-7aa840d191ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dls2022]",
   "language": "python",
   "name": "conda-env-dls2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
